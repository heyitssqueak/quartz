---
title: "CS 544"
---

# resources & deployment

## intro
- Big 3 categories of software:
    1. Analysis code -- running Python code once on some data to get results
    2. Application -- continuously running the code, like a website
    3. Systems -- how to manage resources such as storage space
- Systems resources:
    - compute -> CPU, GPU
        - CPUs can each have multiple cores, which can be independently executing "core code" (machine code)
        - More cores means that more tasks can be run simultaneously
        - High level code -> compiled to bytecode -> ran by a VM -> VM is running on a core
        - GPU has many cores that are coordinated and slow because they have to run together
        - Cost measurement: FLOPS (floating-point operations per second)
    - mem -> RAM
        - RAM stores bits
        - Small, volatile (lost upon reboot) and fast
    - storage -> HDD, SSD
        - Data read/written in blocks of many bytes/blocks
        - Large, nonvolatile, slow
        - How fast can be read/write data? This is measured in throughput
    - network -> NIC (network interface card)
        - Data access speed can be based on network topology -- which servers are physically closest
- How do we efficiently utilize the four above resources?
    1. Scale UP: get more mem/compute power
    2. Scale OUT: cluster machines (expensive)

---

## deployment 
### (linux)
- `which` where something is installed / located
- shebang line tells you what interpreter should run the code
- `|` pipe character chains output from one program to the input of another program
    - programs don't wait for the previous one to conclude, they use streams
- `>` redirects input/output
    `&> out.txt` sends stdout and stderr to `out.txt`
    `2>` just sends stderr somewhere
- `&` sends program to the background to run asynchronously
- `wc` word count produces `lines words chars`
- `grep` search 
- `find` finds all files wrt current dir
- `>>` append to a file instead of overwriting it with `>`
- `ps` lists running processes
- `kill [id]` kills a process by id
- `pkill [name]` kills a process by name
- `htop` memory usage
- `df -h` storage usage
- `lsof` list open files, shows every file that every program has open
    - `-i tcp` shows which programs are running using tcp

---

## memory resources 
### (caching)
- There is latency when loading data from RAM into CPU, the solution to this is the cache "hot" data
- Caching is a resource tradeoff -- if I cache a file, I avoid rereading from storage, but I'm using up memory
- What do we cache? Data/webpages that we need to access repeatedly
- When do we cache? The first time we read something, it is added to the cache
- When do we remove (evict) from the cache? Depends, there are several policies
    - Why do we evict? Limited cache space
    - Random: remove cache entry at random
    - FIFO (first in, first out): remove whatever has been in the cache the longest
    - LRU (least recently used): remove the cache item that was accessed the longest ago
- FIFO and LRU are bad when you need to keep scanning the same data repeatedly because you can have a situation where you evict the values that you need next
    - Cache size = 4, Data = [1,2,3,4,5,1,2,3,4,5], then the hit rate is 0%
- Avg latency = (hit% * hit latency) + (miss% + miss latency)

---

## compute resources 
### (pytorch numbers)
- Specify exactly how many bits are used for the numbers we're working with (uint8, float32, etc)
    - Tradeoffs: precision, range, memory
- Numbers in PyTorch are called "tensors"
- Ints will overflow/underflow, floats will return as inf or nan
- Sigmoid function maps to 0 or 1
- A PyTorch model (`torch.nn.Linear`) is like a function, it can be called on tensor data
- Callable objects in Python
    - Use the `__call__` function to define what happens if that object is used as a function
```python
class Mult:
    def __init__(self, factor):
        self.factor = factor

    def __call__(self, number):
        return number * self.factor

double = Mult(2)
double(10) # prints 20

triple = Mult(3)
triple(5) # prints 15
```


### (pytorch optimization)
- Gradient is the slope at a particular location (in a function)
- Stochastic gradient descent: optimization using slopes
```python
x = torch.tensor(0.0, requires_grad=True) # init at x = 0.0
optimizer = torch.optim.SGD([x], lr=0.01) # optimize x values with learning rate 0.01

for epoch in range(5):
    y = f(x) # apply f to get new y value
    y.backward() # figuring out gradient wrt y -> y/x

    optimizer.step() # makes a change to variables based on gradient and learning rate
    optimizer.zero_grad() # resets gradient to 0 because `.step()` adds to the current gradient value
```
- Learning rate is hard to optimize -- too big and you will miss the target, too small and it will take too long to solve


### (pytorch ml)
- Random split
    - Randomly split into train, validation, and test data sets
    - Why might a model score worse on test data than on validation data? Because we chose the model that fits the best to the validation set.
- Deep learning uses models that are deeply nested functions
$$y = \text{model}(x) = L_N(R(L_{N-1}(R(\cdots(L_1(x))))))$$
- Each $L_k(x)$ stands for a model represented by $L_k(x) = xW+b$ while $R$ stands for a function like sigmoid or ReLU
- PyTorch helps us track computations through DAGs
![ML DAG](/images/image-1.png)
- Loss (Mean Squared Error) depends on a lot of things, so loss is what we try to optimize
- Stochastic gradient descent is doing gradient descent in shuffled batches so that we can minimize issues with not enough RAM
- The `df.pivot` function can reformat tables!
- To use PyTorch for ML, we want to have a DataSet (ds) and a DataLoader (dl)
    - The DataSet is a clean, raw representation of the data that we are using
    - The DataLoader basically helps enumerate and access the DataSet
        - Can help with creating batches and shuffling
```python {title="basic training loop", linenos=false}
model = torch.nn.Linear(1, 1)
optimizer = torch.optim.SGD([model.weight, model.bias], lr=0.00001)
loss_fn = torch.nn.MSELoss()

for epoch in range(50):
    for batchx, batchy in dl:
        predictedy = model(batchx)
        loss = loss_fn(batchy, predictedy)
        loss.backward()   # update weight.grad and bias.grad
        optimizer.step()  # update weight and bias based on gradients
        optimizer.zero_grad()  # weight.grad = 0 and bias.grad = 0

    # how well are we doing?
    x, y = ds[:]
    print(epoch, loss_fn(y, model(x)))
```


### (threads)
- Simple Python programs use at most ONE core so speed is capped by how much work one core can do
- A **process** is a running program which has its own **virtual address space (VAS)**
    - The process cannot directly access physical memory
    - A VAS can have holes
![Processes and address spaces](/images/image-2.png)
- A CPU core can be pointed at one instruction in the code at any time
    - Each core has its own cache
- Threads have their own instruction pointers and stacks, but share the heap
    - Single-threaded processes have one pointer, multi-threaded processes have multiple pointers
- Context switch: switching between threads -- doing it too much is slow
![Context switch](/images/image-3.png)
- Race conditions: when two threads both access a shared variable and problems occur


### (locks)
- Two threads can be interleaved and executed in a way that causes information to be lost
    - Modifying global variables and loading/storing simultaneously
- We want atomicity -- if it happens, then it happens together (otherwise not at all)
- A **lock** (`threading.Lock()`) is held by only one thread at a time and protects critical sections from being inturrupted
    - Putting locks in the wrong places can still cause errors so the importantthing is to be careful / generous with where you're putting locks
    - Locking and releasing is also an expensive operation!
- If an exception occurs before the lock is released, then the code will never execute because the lock is still active
    - Use `with` statements

---

## network resources
### (RPC)
- RPC stands for Remote Procedure Call




# further reading
- Linear Algebra and Learning From Data by Gilbert Strang
- Machine Learning with PyTorch and Scikit-Learn